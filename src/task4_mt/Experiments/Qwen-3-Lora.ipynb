{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c70250-3633-4db2-bc19-cdff5b056d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa04cd1-6732-4179-8e7b-98e7cf61358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url_english = \"https://drive.google.com/uc?export=download&id=1J5zhBA3qvKFPbADziKRv1xBQ9A_jMdRs\"\n",
    "text = requests.get(url_english).text\n",
    "lines = text.splitlines()\n",
    "df_english = pd.DataFrame({\"text\": lines})\n",
    "url_indo = \"https://drive.google.com/uc?export=download&id=1_jMC6ImrPz2KJzj4aWFdTiPS3_2-DcG6\"\n",
    "text = requests.get(url_indo).text\n",
    "lines = text.splitlines()\n",
    "df_indo = pd.DataFrame({\"text\": lines})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026be510-d32f-4356-b74e-8f37de5d3061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Saat ini ada mencit umur 4 bulan nondiabetes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Ehud Ur, profesor kedokteran di Dalhousie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seperti halnya ahli-ahli lain, dia merasa skep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sara Danius, sekretaris permanen Komite Nobel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Danius berkata, \"Saat ini kita tidak melakuka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sebelumnya, CEO Ring, Jamie Siminoff, berkomen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ia melakukan pembuatan bel pintu dengan teknol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Siminoff mengatakan penjualan meningkat setela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Di akhir tahun 2017, Siminoff muncul di salura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ring juga menyelesaikan sebuah tuntutan hukum ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  “Saat ini ada mencit umur 4 bulan nondiabetes ...\n",
       "1  Dr. Ehud Ur, profesor kedokteran di Dalhousie ...\n",
       "2  Seperti halnya ahli-ahli lain, dia merasa skep...\n",
       "3  Sara Danius, sekretaris permanen Komite Nobel ...\n",
       "4  \"Danius berkata, \"Saat ini kita tidak melakuka...\n",
       "5  Sebelumnya, CEO Ring, Jamie Siminoff, berkomen...\n",
       "6  Ia melakukan pembuatan bel pintu dengan teknol...\n",
       "7  Siminoff mengatakan penjualan meningkat setela...\n",
       "8  Di akhir tahun 2017, Siminoff muncul di salura...\n",
       "9  Ring juga menyelesaikan sebuah tuntutan hukum ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2871370-df86-4a4e-bf66-e92105d39de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=device\n",
    "# ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a4a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W1124 03:43:29.247000 25668 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:804: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e79d0504124f48b53da093d20b4b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\config.py:162: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'corda_config', 'ensure_weight_tying', 'peft_version', 'qalora_group_size', 'target_parameters', 'trainable_token_indices', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Qwen LoRA model successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LORA_DIR = \"QWEN3\"   \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_DIR,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded Qwen LoRA model successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7471b54-49a3-4369-bf97-bd2fcee5c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT = \"\"\"\n",
    "English: In 2010, South Korea experienced a particularly cold winter.\n",
    "Indonesian: Pada 2010, Korea Selatan mengalami musim dingin yang tidak biasa.\n",
    "\n",
    "English: People couldn't activate their smartphones while wearing gloves, \n",
    "so they began wielding snack sausages—causing one company to see a 40% rise in sausage sales.\n",
    "Indonesian: Orang-orang tidak dapat menyalakan ponsel pintar mereka saat mengenakan sarung tangan, \n",
    "jadi mereka mulai menggunakan camilan sosis, yang menyebabkan satu perusahaan mengalami kenaikan penjualan sosis sebesar 40%.\n",
    "\"\"\".strip()\n",
    "\n",
    "def prompt_zero_shot(text):\n",
    "    return f\"\"\"Translate to Indonesian:\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_few_shot(text):\n",
    "    return f\"\"\"Translate English to Indonesian following the examples.\n",
    "\n",
    "{FEW_SHOT}\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_role_based(text):\n",
    "    return f\"\"\"You are a professional bilingual educator who creates flashcards.\n",
    "Translate the English sentence into natural Indonesian.\n",
    "Use complete sentences. No explanation. Output ONLY Indonesian.\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_cot_guarded(text):\n",
    "    return f\"\"\"Translate the English sentence into Indonesian.\n",
    "First think through the meaning step-by-step to ensure accuracy.\n",
    "Then provide the final Indonesian translation ONLY in the last line, prefixed with \"Indonesian:\".\n",
    "\n",
    "English: \"{text}\"\n",
    "\n",
    "Reasoning:\n",
    "\"\"\".strip()\n",
    "\n",
    "def prompt_back_translation(text):\n",
    "    return f\"\"\"Step 1: Translate the English sentence to Indonesian.\n",
    "Step 2: Translate that Indonesian back to English to verify consistency.\n",
    "Step 3: Output ONLY the final Indonesian translation in the last line, prefixed with \"Final Indonesian:\".\n",
    "\n",
    "English: \"{text}\"\n",
    "\n",
    "Indonesian:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900b4a8-32c6-480b-972c-87e2eb4f45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_back_translation(gen):\n",
    "    if not isinstance(gen, str):\n",
    "        return \"\"\n",
    "    m = re.search(r'Final Indonesian\\s*:\\s*\"([^\"]+)\"', gen, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    m2 = re.search(r'Indonesian\\s*:\\s*\"([^\"]+)\"', gen, flags=re.I)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r'\"([^\"]+)\"', gen)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return gen.split(\"\\n\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cbd3f-6470-4e8a-b88a-fd66ad36fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_wrappers(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = s.lstrip(\"['\").rstrip(\"']\")\n",
    "    s = s.strip()\n",
    "    s = s.strip('\"').strip(\"'\").strip()\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fe89ff-2bea-4ea0-9c84-ec5942d76207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=60):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def extract_translation(full_output, prompt, method):\n",
    "    gen = full_output.replace(prompt, \"\").strip()\n",
    "\n",
    "    gen = re.sub(r\"\\r\\n?\", \"\\n\", gen).strip()\n",
    "\n",
    "    if method in [\"zero_shot\", \"few_shot\", \"role_based\"]:\n",
    "        lines = [l.strip() for l in gen.split(\"\\n\") if l.strip()]\n",
    "        if not lines:\n",
    "            return \"\"\n",
    "        m = re.match(r'^(Indonesian|Indonesia|Terjemahan|Translation)\\s*:\\s*(.*)$', lines[0], re.I)\n",
    "        return strip_wrappers((m.group(2).strip() if m else lines[0]))\n",
    "    if method == \"back_translation\":\n",
    "        return strip_wrappers(extract_back_translation(gen))\n",
    "    return gen.split(\"\\n\")[0].strip()\n",
    "\n",
    "def generate_only_translation(prompt, method, max_new_tokens=160):\n",
    "    full_output = generate_text(prompt, max_new_tokens=max_new_tokens)\n",
    "    return extract_translation(full_output, prompt, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca9e7c3-f8e6-43fe-a79a-d4b92238a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_bleu(ref, hypo):\n",
    "    return sentence_bleu([ref.split()], hypo.split(), smoothing_function=smooth)\n",
    "\n",
    "def compute_rougeL(ref, hypo):\n",
    "    return rouge.score(ref, hypo)[\"rougeL\"].fmeasure\n",
    "\n",
    "def compute_bertscore(ref, hypo):\n",
    "    P, R, F1 = bertscore([hypo], [ref], lang=\"id\")\n",
    "    return F1[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a68e4-1df1-461f-b352-5e17ccba0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_pipeline(df_en, df_id, method_name):\n",
    "    methods = {\n",
    "        \"zero_shot\": prompt_zero_shot,\n",
    "        \"few_shot\": prompt_few_shot,\n",
    "        \"role_based\": prompt_role_based,\n",
    "        \"back_translation\": prompt_back_translation,\n",
    "    }\n",
    "\n",
    "    if method_name not in methods:\n",
    "        raise ValueError(f\"Invalid method. Choose from: {list(methods.keys())}\")\n",
    "\n",
    "    method_func = methods[method_name]\n",
    "\n",
    "    all_outputs = []\n",
    "    bleu_list, rouge_list, bert_list = [], [], []\n",
    "\n",
    "    print(f\"\\n=== RUNNING {method_name.upper()} (NO BATCHING) ===\")\n",
    "\n",
    "    for i in tqdm(range(len(df_en))):\n",
    "\n",
    "        en = df_en.loc[i, \"text\"]\n",
    "        gt = df_id.loc[i, \"text\"]\n",
    "\n",
    "        prompt = method_func(en)\n",
    "\n",
    "        full_output = generate_text(prompt, max_new_tokens=60)\n",
    "        pred = extract_translation(full_output, prompt, method_name)\n",
    "\n",
    "        all_outputs.append(pred)\n",
    "\n",
    "        bleu_list.append(compute_bleu(gt, pred))\n",
    "        rouge_list.append(compute_rougeL(gt, pred))\n",
    "        bert_list.append(compute_bertscore(gt, pred))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    scores = {\n",
    "        \"bleu\": float(np.mean(bleu_list)),\n",
    "        \"rougeL\": float(np.mean(rouge_list)),\n",
    "        \"bertscore\": float(np.mean(bert_list)),\n",
    "    }\n",
    "\n",
    "    return all_outputs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892646cc-dd6a-4906-bb5f-54c8c11a536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "def generate_batch(prompts, max_new_tokens=60):\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=False,    \n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06d26a-83e6-4f43-b947-5904088c7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batched_pipeline(df_en, df_id, method_name, batch_size=4):\n",
    "    methods = {\n",
    "        \"zero_shot\": prompt_zero_shot,\n",
    "        \"few_shot\": prompt_few_shot,\n",
    "        \"role_based\": prompt_role_based,\n",
    "        \"back_translation\": prompt_back_translation,\n",
    "    }\n",
    "\n",
    "    if method_name not in methods:\n",
    "        raise ValueError(f\"Invalid method. Choose from: {list(methods.keys())}\")\n",
    "\n",
    "    method_func = methods[method_name]\n",
    "\n",
    "    all_outputs = []\n",
    "    bleu_list, rouge_list, bert_list = [], [], []\n",
    "\n",
    "    print(f\"\\n=== RUNNING {method_name.upper()} | BATCH SIZE = {batch_size} ===\")\n",
    "\n",
    "    num_samples = len(df_en)\n",
    "\n",
    "    for start in tqdm(range(0, num_samples, batch_size)):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "\n",
    "        en_batch = [df_en.loc[i, \"text\"] for i in range(start, end)]\n",
    "        gt_batch = [df_id.loc[i, \"text\"] for i in range(start, end)]\n",
    "\n",
    "        prompts = [method_func(en) for en in en_batch]\n",
    "\n",
    "        raw_outputs = generate_batch(prompts, max_new_tokens=60)\n",
    "\n",
    "        preds = [\n",
    "            extract_translation(raw_outputs[j], prompts[j], method_name)\n",
    "            for j in range(len(raw_outputs))\n",
    "        ]\n",
    "\n",
    "        all_outputs.extend(preds)\n",
    "        for j in range(len(preds)):\n",
    "            bleu_list.append(compute_bleu(gt_batch[j], preds[j]))\n",
    "            rouge_list.append(compute_rougeL(gt_batch[j], preds[j]))\n",
    "            bert_list.append(compute_bertscore(gt_batch[j], preds[j]))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    scores = {\n",
    "        \"bleu\": float(np.mean(bleu_list)),\n",
    "        \"rougeL\": float(np.mean(rouge_list)),\n",
    "        \"bertscore\": float(np.mean(bert_list)),\n",
    "    }\n",
    "\n",
    "    return all_outputs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515ac48e-3932-4bb8-bf0e-195f29108fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_methods(df_en, df_id, batch_size=4):\n",
    "    methods = [\"zero_shot\", \"few_shot\", \"role_based\", \"back_translation\"]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for m in methods:\n",
    "        print(f\"\\n========== Evaluating {m.upper()} ==========\")\n",
    "        outputs, scores = run_batched_pipeline(df_en, df_id, m, batch_size=batch_size)\n",
    "        results[m] = scores\n",
    "        print(scores)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3841b6cb-6caa-4479-8d29-d383fb28b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Evaluating ZERO_SHOT ==========\n",
      "\n",
      "=== RUNNING ZERO_SHOT | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 169/169 [1:19:49<00:00, 28.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.1366329578618148, 'rougeL': 0.46171740537424516, 'bertscore': 0.7950485877015374}\n",
      "\n",
      "========== Evaluating FEW_SHOT ==========\n",
      "\n",
      "=== RUNNING FEW_SHOT | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [2:33:04<00:00, 54.35s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.14067391871241725, 'rougeL': 0.4749766871682004, 'bertscore': 0.7970898681478538}\n",
      "\n",
      "========== Evaluating ROLE_BASED ==========\n",
      "\n",
      "=== RUNNING ROLE_BASED | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [1:15:33<00:00, 26.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.19901699313059873, 'rougeL': 0.5848279779225082, 'bertscore': 0.8425409903992778}\n",
      "\n",
      "========== Evaluating BACK_TRANSLATION ==========\n",
      "\n",
      "=== RUNNING BACK_TRANSLATION | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 18/169 [09:54<1:25:29, 33.97s/it]HTTP Error 504 thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 504 thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "100%|██████████| 169/169 [1:35:13<00:00, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.12518495638836832, 'rougeL': 0.41674269892308535, 'bertscore': 0.7660905435504649}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_methods(df_english, df_indo, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37651e7c-1fe0-4de1-8c20-6c9b7341ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_zero, scores_zero = run_single_pipeline(df_english, df_indo, \"zero_shot\")\n",
    "# print(scores_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dcc1926-37cf-4c19-b413-e31c7650058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_few, scores_few = run_single_pipeline(df_english, df_indo, \"few_shot\")\n",
    "# print(scores_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a628dfa-2918-4b1b-9f0a-4478a570be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_role, scores_role = run_single_pipeline(df_english, df_indo, \"role_based\")\n",
    "# print(scores_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29efd726-8848-4201-ac2a-8dab3e8add46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_back, scores_back = run_single_pipeline(df_english, df_indo, \"back_translation\")\n",
    "# print(scores_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e72017b-1de4-4e05-b342-f672d24744c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_one_en = df_english.iloc[[0]]\n",
    "# df_one_id = df_indo.iloc[[0]]\n",
    "\n",
    "# outputs, scores = run_batched_pipeline(df_one_en, df_one_id, \"zero_shot\", batch_size=1)\n",
    "# print(outputs)\n",
    "# print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
