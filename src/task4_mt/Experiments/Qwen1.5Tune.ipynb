{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c422fa3-5e8f-4067-a32e-4847116b81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74b534b-3f91-40a6-8058-b918d1d9dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url_english = \"https://drive.google.com/uc?export=download&id=1J5zhBA3qvKFPbADziKRv1xBQ9A_jMdRs\"\n",
    "text = requests.get(url_english).text\n",
    "lines = text.splitlines()\n",
    "df_english = pd.DataFrame({\"text\": lines})\n",
    "url_indo = \"https://drive.google.com/uc?export=download&id=1_jMC6ImrPz2KJzj4aWFdTiPS3_2-DcG6\"\n",
    "text = requests.get(url_indo).text\n",
    "lines = text.splitlines()\n",
    "df_indo = pd.DataFrame({\"text\": lines})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4a26a-774b-41b8-b23c-99bfaaab5b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Qwen LoRA model successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "LORA_DIR = \"qwen1.5b-lora-translation/qwen1.5\"   \n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_DIR,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded Qwen LoRA model successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15a5ed6-aea7-49aa-bf7a-50619eb4f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT = \"\"\"\n",
    "English: In 2010, South Korea experienced a particularly cold winter.\n",
    "Indonesian: Pada 2010, Korea Selatan mengalami musim dingin yang tidak biasa.\n",
    "\n",
    "English: People couldn't activate their smartphones while wearing gloves, \n",
    "so they began wielding snack sausages—causing one company to see a 40% rise in sausage sales.\n",
    "Indonesian: Orang-orang tidak dapat menyalakan ponsel pintar mereka saat mengenakan sarung tangan, \n",
    "jadi mereka mulai menggunakan camilan sosis, yang menyebabkan satu perusahaan mengalami kenaikan penjualan sosis sebesar 40%.\n",
    "\"\"\".strip()\n",
    "\n",
    "def prompt_zero_shot(text):\n",
    "    return f\"\"\"Translate to Indonesian:\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_few_shot(text):\n",
    "    return f\"\"\"Translate English to Indonesian following the examples.\n",
    "\n",
    "{FEW_SHOT}\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_role_based(text):\n",
    "    return f\"\"\"You are a professional bilingual educator who creates flashcards.\n",
    "Translate the English sentence into natural Indonesian.\n",
    "Use complete sentences. No explanation. Output ONLY Indonesian.\n",
    "\n",
    "English: \"{text}\"\n",
    "Indonesian:\"\"\".strip()\n",
    "\n",
    "def prompt_cot_guarded(text):\n",
    "    return f\"\"\"Translate the English sentence into Indonesian.\n",
    "First think through the meaning step-by-step to ensure accuracy.\n",
    "Then provide the final Indonesian translation ONLY in the last line, prefixed with \"Indonesian:\".\n",
    "\n",
    "English: \"{text}\"\n",
    "\n",
    "Reasoning:\n",
    "\"\"\".strip()\n",
    "\n",
    "def prompt_back_translation(text):\n",
    "    return f\"\"\"Step 1: Translate the English sentence to Indonesian.\n",
    "Step 2: Translate that Indonesian back to English to verify consistency.\n",
    "Step 3: Output ONLY the final Indonesian translation in the last line, prefixed with \"Final Indonesian:\".\n",
    "\n",
    "English: \"{text}\"\n",
    "\n",
    "Indonesian:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a835a1-e2c6-413f-87cc-adeb3a2ade1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_back_translation(gen):\n",
    "    if not isinstance(gen, str):\n",
    "        return \"\"\n",
    "\n",
    "    m = re.search(r'Final Indonesian\\s*:\\s*\"([^\"]+)\"', gen, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    m2 = re.search(r'Indonesian\\s*:\\s*\"([^\"]+)\"', gen, flags=re.I)\n",
    "    if m2:\n",
    "        return m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r'\"([^\"]+)\"', gen)\n",
    "    if m3:\n",
    "        return m3.group(1).strip()\n",
    "\n",
    "    return gen.split(\"\\n\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee4ed6-bbe3-458f-979f-2389c45bc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_wrappers(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = s.lstrip(\"['\").rstrip(\"']\")\n",
    "    s = s.strip()\n",
    "    s = s.strip('\"').strip(\"'\").strip()\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a777615e-5806-4110-9da0-964b0207801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=60):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def extract_translation(full_output, prompt, method):\n",
    "    gen = full_output.replace(prompt, \"\").strip()\n",
    "\n",
    "    gen = re.sub(r\"\\r\\n?\", \"\\n\", gen).strip()\n",
    "\n",
    "    if method in [\"zero_shot\", \"few_shot\", \"role_based\"]:\n",
    "        lines = [l.strip() for l in gen.split(\"\\n\") if l.strip()]\n",
    "        if not lines:\n",
    "            return \"\"\n",
    "        m = re.match(r'^(Indonesian|Indonesia|Terjemahan|Translation)\\s*:\\s*(.*)$', lines[0], re.I)\n",
    "        return strip_wrappers((m.group(2).strip() if m else lines[0]))\n",
    "    if method == \"back_translation\":\n",
    "        return strip_wrappers(extract_back_translation(gen))\n",
    "    return gen.split(\"\\n\")[0].strip()\n",
    "\n",
    "def generate_only_translation(prompt, method, max_new_tokens=160):\n",
    "    full_output = generate_text(prompt, max_new_tokens=max_new_tokens)\n",
    "    return extract_translation(full_output, prompt, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e720df-8f8f-4c4d-b793-91b3152007d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = SmoothingFunction().method1\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_bleu(ref, hypo):\n",
    "    return sentence_bleu([ref.split()], hypo.split(), smoothing_function=smooth)\n",
    "\n",
    "def compute_rougeL(ref, hypo):\n",
    "    return rouge.score(ref, hypo)[\"rougeL\"].fmeasure\n",
    "\n",
    "def compute_bertscore(ref, hypo):\n",
    "    P, R, F1 = bertscore([hypo], [ref], lang=\"id\")\n",
    "    return F1[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b5edf-18a0-492d-aea0-e5438fe324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_pipeline(df_en, df_id, method_name):\n",
    "    methods = {\n",
    "        \"zero_shot\": prompt_zero_shot,\n",
    "        \"few_shot\": prompt_few_shot,\n",
    "        \"role_based\": prompt_role_based,\n",
    "        \"back_translation\": prompt_back_translation,\n",
    "    }\n",
    "\n",
    "    if method_name not in methods:\n",
    "        raise ValueError(f\"Invalid method. Choose from: {list(methods.keys())}\")\n",
    "\n",
    "    method_func = methods[method_name]\n",
    "\n",
    "    all_outputs = []\n",
    "    bleu_list, rouge_list, bert_list = [], [], []\n",
    "\n",
    "    print(f\"\\n=== RUNNING {method_name.upper()} (NO BATCHING) ===\")\n",
    "\n",
    "    for i in tqdm(range(len(df_en))):\n",
    "\n",
    "        en = df_en.loc[i, \"text\"]\n",
    "        gt = df_id.loc[i, \"text\"]\n",
    "\n",
    "        prompt = method_func(en)\n",
    "\n",
    "        full_output = generate_text(prompt, max_new_tokens=60)\n",
    "        pred = extract_translation(full_output, prompt, method_name)\n",
    "\n",
    "        all_outputs.append(pred)\n",
    "\n",
    "        bleu_list.append(compute_bleu(gt, pred))\n",
    "        rouge_list.append(compute_rougeL(gt, pred))\n",
    "        bert_list.append(compute_bertscore(gt, pred))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    scores = {\n",
    "        \"bleu\": float(np.mean(bleu_list)),\n",
    "        \"rougeL\": float(np.mean(rouge_list)),\n",
    "        \"bertscore\": float(np.mean(bert_list)),\n",
    "    }\n",
    "\n",
    "    return all_outputs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec31d00-ff2c-4a5b-ac9c-92f2d4f3f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "def generate_batch(prompts, max_new_tokens=60):\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=False,   \n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4b750-e6f5-422f-b3cc-784c8f955906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batched_pipeline(df_en, df_id, method_name, batch_size=4):\n",
    "    methods = {\n",
    "        \"zero_shot\": prompt_zero_shot,\n",
    "        \"few_shot\": prompt_few_shot,\n",
    "        \"role_based\": prompt_role_based,\n",
    "        \"back_translation\": prompt_back_translation,\n",
    "    }\n",
    "\n",
    "    if method_name not in methods:\n",
    "        raise ValueError(f\"Invalid method. Choose from: {list(methods.keys())}\")\n",
    "\n",
    "    method_func = methods[method_name]\n",
    "\n",
    "    all_outputs = []\n",
    "    bleu_list, rouge_list, bert_list = [], [], []\n",
    "\n",
    "    print(f\"\\n=== RUNNING {method_name.upper()} | BATCH SIZE = {batch_size} ===\")\n",
    "\n",
    "    num_samples = len(df_en)\n",
    "\n",
    "    for start in tqdm(range(0, num_samples, batch_size)):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "\n",
    "        en_batch = [df_en.loc[i, \"text\"] for i in range(start, end)]\n",
    "        gt_batch = [df_id.loc[i, \"text\"] for i in range(start, end)]\n",
    "\n",
    "        prompts = [method_func(en) for en in en_batch]\n",
    "\n",
    "        raw_outputs = generate_batch(prompts, max_new_tokens=60)\n",
    "\n",
    "        preds = [\n",
    "            extract_translation(raw_outputs[j], prompts[j], method_name)\n",
    "            for j in range(len(raw_outputs))\n",
    "        ]\n",
    "\n",
    "        all_outputs.extend(preds)\n",
    "\n",
    "        for j in range(len(preds)):\n",
    "            bleu_list.append(compute_bleu(gt_batch[j], preds[j]))\n",
    "            rouge_list.append(compute_rougeL(gt_batch[j], preds[j]))\n",
    "            bert_list.append(compute_bertscore(gt_batch[j], preds[j]))\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    scores = {\n",
    "        \"bleu\": float(np.mean(bleu_list)),\n",
    "        \"rougeL\": float(np.mean(rouge_list)),\n",
    "        \"bertscore\": float(np.mean(bert_list)),\n",
    "    }\n",
    "\n",
    "    return all_outputs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf6d539-a221-4cc9-9ff4-d87931df4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_methods(df_en, df_id, batch_size=4):\n",
    "    methods = [\"zero_shot\", \"few_shot\", \"role_based\", \"back_translation\"]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for m in methods:\n",
    "        print(f\"\\n========== Evaluating {m.upper()} ==========\")\n",
    "        outputs, scores = run_batched_pipeline(df_en, df_id, m, batch_size=batch_size)\n",
    "        results[m] = scores\n",
    "        print(scores)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f8cc07-37c4-4b0a-bea9-aafa01cc0aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Evaluating ZERO_SHOT ==========\n",
      "\n",
      "=== RUNNING ZERO_SHOT | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3dbe65c54f44b5a68e1e7f27fb2d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07274db83c2c4ce3a8e6971557a16a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e98577d124c1180ced222a199d583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1069e3d1dd440df9f0a1ffba02f0511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746e6f35fe6c49e1baf896079bcf2ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [22:08<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.138343503103525, 'rougeL': 0.5123305885823051, 'bertscore': 0.8228814369251606}\n",
      "\n",
      "========== Evaluating FEW_SHOT ==========\n",
      "\n",
      "=== RUNNING FEW_SHOT | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [25:56<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.134361424339305, 'rougeL': 0.5139593469003865, 'bertscore': 0.8250882923014079}\n",
      "\n",
      "========== Evaluating ROLE_BASED ==========\n",
      "\n",
      "=== RUNNING ROLE_BASED | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [26:12<00:00,  9.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.14227376750453122, 'rougeL': 0.518044570907784, 'bertscore': 0.8240086426259029}\n",
      "\n",
      "========== Evaluating BACK_TRANSLATION ==========\n",
      "\n",
      "=== RUNNING BACK_TRANSLATION | BATCH SIZE = 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [27:41<00:00,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.09946334765193332, 'rougeL': 0.4125833164034409, 'bertscore': 0.7626790563932991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_methods(df_english, df_indo, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cb4d6-0fc4-4471-9965-4ee038022bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
