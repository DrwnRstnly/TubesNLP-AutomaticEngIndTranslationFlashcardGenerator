{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0282fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W1126 22:28:32.028000 42252 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b3e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_text(text):\n",
    "    text = re.sub(r\"^(ID|EN|To|From)\\s*:\\s*\", \"\", text, flags=re.I)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:6105: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ae9ced28504f0e8b5a96e28fe251f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\config.py:162: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'corda_config', 'ensure_weight_tying', 'peft_version', 'qalora_group_size', 'target_parameters', 'trainable_token_indices', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: The auditors concluded that the financial discrepancies did not stem from intentional misreporting, but from a series of legacy processes that were never fully integrated during the previous restructuring.\n",
      "ID: Auditernya menutup bahwa perbedaan keuangan tidak berasal dari penyebaran yang sengaja, tetapi dari sejumlah proses legi yang tidak pernah sepenuhnya terintegrasi selama restructuring sebelumnya\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device\n",
    ")\n",
    "lora_path = \"./QWEN3\"  \n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "def prompt_role_based(text, domain=None):\n",
    "    domain_info = f\"({domain})\" if domain else \"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "Translate the English sentence to Indonesian {domain_info}.\n",
    "This is a direct mapping task. No reasoning. No explanation.\n",
    "Do not modify meaning. Do not invert comparative direction.\n",
    "cheaper → lebih murah\n",
    "more expensive → lebih mahal\n",
    "\n",
    "Return ONLY the Indonesian translation.\n",
    "\n",
    "English: {text}\n",
    "Indonesian:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def strip_wrappers(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"^\\[\\s*[\\\"'](.*?)[\\\"']\\s*\\]$\", r\"\\1\", s)\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_new_tokens=60):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            top_p=1.0\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def extract_translation(full_output, prompt):\n",
    "    gen = full_output.replace(prompt, \"\").strip()\n",
    "    if \"Indonesian:\" in gen:\n",
    "        gen = gen.split(\"Indonesian:\")[1].strip()\n",
    "    sentence = gen.split(\".\")[0].strip()\n",
    "    return sentence\n",
    "\n",
    "def translate_role(text, domain=None):\n",
    "    text = clean_input_text(text)   \n",
    "    prompt = prompt_role_based(text, domain=domain)\n",
    "    output = generate_text(prompt, max_new_tokens=160)\n",
    "    return extract_translation(output, prompt)\n",
    "\n",
    "english_sentence = \"The auditors concluded that the financial discrepancies did not stem from intentional misreporting, but from a series of legacy processes that were never fully integrated during the previous restructuring.\"\n",
    "result = translate_role(english_sentence, domain=\"audit\")\n",
    "print(\"EN:\", english_sentence)\n",
    "print(\"ID:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
